#!/usr/bin/env python3
"""usage: connectome_precomputed [-h] -r -c -o

Generate AvgR, AvgR_Fz, and T functional connectivity chunks for a single chunk.

Arguments:
  -h, --help        show this help message and exit

  -r, --roi-dir     Path to directory containing binary Nifti ROIs, or path to a single 
                    binary Nifti ROI, or path to CSV containing binary Nifti ROI paths.
                    If using S3 storage, this should be an S3 path in the format 
                    s3://bucket-name/prefix

  -c, --config      Name of precomputed connectome config file to use.

  -o, --output-dir  Path to output directory.
                    If using S3 storage, this should be an S3 path in the format 
                    s3://bucket-name/prefix
"""

import os
import argparse
from tqdm import tqdm
import nibabel as nib
from pfctoolkit import tools
from pfctoolkit import config
from pfctoolkit import mapping
from pfctoolkit import datasets
from pfctoolkit import s3_tools

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate functional lesion network maps using the precomputed connectome."
    )

    parser.add_argument(
        "-r",
        "--roi-dir",
        metavar="\b",
        help="Path to directory containing binary Nifti ROIs, or path to a single binary Nifti ROI, or path to CSV containing binary Nifti ROI paths.",
        type=str,
        required=True,
    )

    parser.add_argument(
        "-c",
        "--config",
        metavar="\b",
        help="Name of precomputed connectome config file to use.",
        type=str,
        required=True,
    )

    parser.add_argument(
        "-o",
        "--output-dir",
        metavar="\b",
        help="Path to output directory.",
        type=str,
        required=True,
    )

    # Parse arguments
    args = parser.parse_args()

    # Load and check PCC configuration
    pcc_config = config.Config(args.config)

    # Load s3 storage object (if needed)
    if pcc_config.get("USE_S3") == True:
        s3_storage = s3_tools.S3Storage(pcc_config)
        roi_dir = args.roi_dir  # Keep the S3 path as is
    else:
        s3_storage = None
        roi_dir = os.path.abspath(args.roi_dir)  # Convert to absolute path for local files

    # Load ROI list
    roi_paths = tools.load_roi(roi_dir, s3_storage)

    # Set output directory
    if pcc_config.get("USE_S3") == True:
        output_dir = args.output_dir # Keep the S3 path as is
    else: 
        output_dir = os.path.abspath(args.output_dir) # Convert to absolute path for local files

    chunks = tools.get_chunks(roi_paths, pcc_config, s3_storage)

    # Process Chunks
    atlas = {}
    for chunk in tqdm(chunks):
        contribution = mapping.process_chunk(chunk, chunks[chunk], pcc_config, s3_storage)
        atlas = mapping.update_atlas(contribution, atlas)

    # Consolidate outputs
    mapping.publish_atlas(atlas, output_dir, pcc_config, s3_storage)
